// Example 14: Larger synthetic linear regression (20 samples)
// Fit W to minimize mean squared error: Y = X @ W
// True weights: [1.5, 2.0]

fn main() {
    // 20 samples, 2 features
    let X = tensor [
        [1.0, 1.0], [1.0, 2.0], [2.0, 1.0], [3.0, 2.0],
        [2.0, 3.0], [3.0, 1.0], [4.0, 2.0], [5.0, 3.0],
        [6.0, 2.0], [7.0, 3.0], [8.0, 4.0], [9.0, 5.0],
        [10.0, 6.0], [2.5, 1.5], [3.5, 2.5], [4.5, 3.5],
        [5.5, 3.5], [6.5, 4.5], [7.5, 5.5], [8.5, 6.5]
    ];

    // Target: Y = 1.5*X1 + 2.0*X2 (exact values)
    let T = tensor [
        [3.5], [5.5], [5.0], [8.5],
        [9.0], [6.5], [10.0], [13.5],
        [13.0], [16.5], [20.0], [23.5],
        [27.0], [6.75], [10.25], [13.75],
        [15.25], [18.75], [22.25], [25.75]
    ];

    // Learnable weights (2x1)
    learn W = tensor [[0.0], [0.0]];

    let learning_rate = 0.001;
    let max_iterations = 100000;

    optimize(W) until loss < 0.001 {
        let Y = matmul(X, W);          // (20x1)
        let E = Y - T;                 // (20x1)
        let loss = mean(E * E);        // scalar
        minimize loss;
    }

    // Final check
    let Yf = matmul(X, W);
    let final_loss = mean((Yf - T) * (Yf - T));
    print(final_loss);
    return final_loss;
}
